Title: Definiciones y Herramientas
Date: 2023-03-10 00:27
Category: 2. Teórico

## Que es machine learning:

# Definiciones:

El machine learning es una rama de la inteligencia artificial (IA) y la informática que se centra en el uso de datos y algoritmos para imitar la forma en que aprenden los humanos, mejorando gradualmente su precisión.

El aprendizaje automático (machine learning en inglés) es un campo de la inteligencia artificial que se enfoca en el desarrollo de algoritmos y modelos que permiten a las computadoras aprender de datos pasados para realizar tareas específicas sin ser programadas explícitamente. En lugar de seguir instrucciones precisas, los sistemas de aprendizaje automático utilizan patrones y ejemplos para mejorar su rendimiento con el tiempo. Estos modelos se aplican en una variedad de áreas, como reconocimiento de patrones, clasificación, predicción y toma de decisiones.

1-

La IA es la capacidad de las computadoras de mostrar un comportamiento inteligente. Mientras que ML es una técnica que se utiliza para crear y mejorar dicho comportamiento.

2-

El análisis estadístico se centra en la interpretación de datos numéricos para extraer información y hacer inferencias probabilísticas, la inteligencia artificial busca crear sistemas que puedan aprender y tomar decisiones a través de algoritmos y modelos computacionales avanzados. Estas dos disciplinas a menudo se complementan entre sí en la era de la informática y el procesamiento de datos.

3-

El aprendizaje automático se centra en construir modelos para hacer predicciones a partir de datos, mientras que la minería de datos se enfoca en descubrir patrones y relaciones en conjuntos de datos sin necesariamente predecir. Ambos buscan conocimiento, pero con enfoques diferentes.

4-
El machine learning se utiliza para enseñar a las máquinas a aprender de datos y mejorar su rendimiento en tareas específicas sin necesidad de programación explícita.

---

# Herramientas

ESTUDIO WATSON DE IBM

https://www.ibm.com/products/watson-studio

Beneficios
Optimice la IA y la economía de la nube
Ponga la IA multinube a trabajar para los negocios. Utilizar modelos de consumo flexibles. Cree e implemente IA en cualquier lugar.

Predecir resultados y prescribir acciones
Optimice los cronogramas, los planes y las asignaciones de recursos mediante predicciones. Simplifique el modelado de optimización con una interfaz de lenguaje natural.

Sincroniza aplicaciones e IA
Unir y capacitar a desarrolladores y científicos de datos. Empuje los modelos a través de la API REST en cualquier nube. Ahorre tiempo y costes gestionando herramientas dispares.

Unifique herramientas y aumente la productividad para ModelOps
Ponga en funcionamiento la IA empresarial en las nubes. Gobierne y asegure proyectos de ciencia de datos a escala.

Ofrezca IA explicable
Reduzca los esfuerzos de monitoreo del modelo entre un 35 % y un 50 %.¹ Aumente la precisión del modelo entre un 15 % y un 30 %.² Aumente las ganancias netas en una plataforma de datos e inteligencia artificial.

Gestionar los riesgos y el cumplimiento normativo
Protéjase contra la exposición y las sanciones reglamentarias. Simplifique la gestión de riesgos del modelo de IA a través de la validación automatizada.

SHOGUN

https://pydata.org/project/shogun/
PyData
dev_nfpd_admin
Shogun
Ventajas de usar Shogun

Es adecuado para la creación rápida de prototipos.
Ofrece características adaptables y orientadas al usuario.
Contras de usar Shogun

No cuenta con el apoyo de la comunidad popular en comparación con TensorFlow o Caff
WEKA

https://www.cs.waikato.ac.nz/ml/weka/

Beneficios

Disponibilidad gratuita bajo la Licencia Pública General GNU.
Portabilidad, ya que está completamente implementado en el lenguaje de programación Java y, por lo tanto, se ejecuta en casi cualquier plataforma informática moderna.
Una colección completa de técnicas de preprocesamiento y modelado de datos.

SAGE MAKER

https://aws.amazon.com/es/sagemaker/

Entorno de trabajo integrado: SageMaker proporciona un entorno unificado que cubre todo el ciclo de vida del aprendizaje automático, desde la preparación y exploración de datos hasta el entrenamiento y la implementación de modelos.

Preparación de datos: Incluye herramientas para cargar, explorar y transformar datos en formatos comunes. Puedes realizar limpieza, transformaciones y manejo de datos faltantes directamente en SageMaker.

Entrenamiento de modelos: SageMaker ofrece un entorno escalable y optimizado para entrenar modelos de aprendizaje automático. Puedes usar algoritmos incorporados, algoritmos personalizados o tus propios contenedores de Docker para entrenar tus modelos.

Optimización automática: Proporciona funcionalidad para optimizar automáticamente los hiperparámetros de tu modelo utilizando técnicas como la búsqueda de hiperparámetros aleatoria o la optimización bayesiana.

RapidMiner

https://rapidminer.com/

RapidMiner es realmente rápido para leer todo tipo de bases de datos. Leemos y fusionamos bases de datos como SQL Server, Informix, MySQL y Oracle. Configurar el acceso es fácil, algunos controladores están integrados, pero no es difícil encontrar nuevos controladores Java para permitir que RapidMiner se conecte a otras bases de datos.
Realizando todo tipo de transformaciones, cálculos (fecha, porcentajes...), uniones y filtros sin programar. Tenemos varias bases de datos diferentes y esto hace mi vida mucho más fácil. Sabiendo que esta parte es el 80% del trabajo del analista, sabe que puede trabajar más en los análisis en sí y no en la limpieza y preparación de datos.
Puede clonar transformaciones para reutilizarlas en nuevos análisis, por lo que ahorra mucho tiempo. Hay muchos complementos para hacer diferentes cosas (texto, análisis de imágenes, sistemas de recomendación, etc.).
La capacitación es fácil, la herramienta es intuitiva y hay muchos videos en Internet. La comunidad es muy activa. (Comentario en una página web de revisión de software)
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

CRISP-DM (Cross-Industry Standard Process for Data Mining) es un modelo estándar utilizado en la minería de datos para guiar el desarrollo de proyectos de análisis de datos. Consiste en seis fases interconectadas: comprensión del negocio, comprensión de los datos, preparación de los datos, modelado, evaluación y despliegue. Este proceso proporciona una estructura sistemática para abordar problemas de minería de datos, desde la comprensión inicial del problema y los datos hasta la implementación de soluciones y la iteración para mejorar el rendimiento del modelo.

Otros procesos similares al CRISP-DM incluyen el proceso KDD (Knowledge Discovery in Databases), que abarca desde la selección de datos hasta la interpretación de resultados; y el proceso TDSP (Team Data Science Process), centrado en proyectos de ciencia de datos en equipo e incorpora iteración continua y colaboración.

CRISP-DM, KDD y TDSP son enfoques para gestionar proyectos de análisis de datos, pero difieren en detalles específicos. CRISP-DM se centra en pasos clave como la comprensión del negocio, preparación de datos, modelado y evaluación, proporcionando una guía amplia y general. KDD (Knowledge Discovery in Databases) abarca la selección de datos, preprocesamiento, transformación y visualización, y está más orientado a la extracción de conocimiento. TDSP (Team Data Science Process) es similar a CRISP-DM pero enfatiza la colaboración en equipo y la iteración continua en proyectos de ciencia de datos. Cada enfoque adapta y ajusta la metodología general a sus objetivos y requisitos específicos.

https://www.gartner.es/es/metodologias/magic-quadrant

---

## Algoritmos Soportados

Rapidminer-

 Clustering:

k means - si - add_cluster_attribute, add_as_label, k, etc
k modes - no
gaussian mixture model - si, con extension smile
dbscan - si - epsilon, minimal_points
Hierarchical - si - hierarchy, use_local_random_seed, local_random_seed

Dimension Reduction -

Latent Dirichlet Analysis - No
Singular Value Decomposition - Si - dimensionality_reduction, percentage_threshold, dimensions
Principal Component Analysis - Si - dimensionality_reduction, variance_threshold, number_of_components

Classification and Regression:

Kernel SVM - Si - kernel_type, kernel_gamma, kernel_sigma1, etc
Linear SVM - No
Random Forest - Si - minimal_size_for_split, minimal_leaf_size ,minimal_gain, etc
Neural Network - Si - hidden_layers, training_cycles, learning_rate, etc
Gradient Boosting Trees - Si - number_of_trees, reproducible, maximum_number_of_threads, etc
Decision Tree - Si - criterion, minimal_size_for_split, minimal_leaf_size, etc
Logistic Regression - Si - kernel_type, kernel_gamma, kernel_sigma1, etc
Naive Bayes - Si - laplace_correction

IBM Watson Studio

Clustering

k means`- si
k modes - no
Hierarchical - No
Gaussian mixture model - Si
dbscan - no

Dimension Reduction

Latent Dirichlet Analysis - No
Singular Value Decomposition - Si
Principal Component Analysis - Si

Classification and Regression:

Kernel SVM - No
Linear SVM - No
Random Forest - Si
Neural Network - Si
Gradient Boosting Tree - No
Decision Tree - Si
Logistic Regression - Si
Naive Bayes  - Si
